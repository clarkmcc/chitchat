[
  {
    "recommended": true,
    "name": "LLaMA 2 Chat",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-2-7b-chat.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q2_K.bin",
    "parameterCount": "7B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "recommended": true,
    "name": "LLaMA 2 Chat",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-2-7b-chat.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_M.bin",
    "parameterCount": "7B",
    "quantization": "4-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "recommended": true,
    "name": "LLaMA 2 Chat",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-2-7b-chat.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin",
    "parameterCount": "7B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "LLaMA",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-7b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q2_K.bin",
    "parameterCount": "7B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "LLaMA",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-7b.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q4_K_M.bin",
    "parameterCount": "7B",
    "quantization": "4-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "LLaMA",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-7b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q8_0.bin",
    "parameterCount": "7B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "LLaMA",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-13b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q2_K.bin",
    "parameterCount": "13B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "LLaMA",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-13b.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q4_K_M.bin",
    "parameterCount": "13B",
    "quantization": "4-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "LLaMA 13B",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-13b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q8_0.bin",
    "parameterCount": "13B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "Vicuna",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "vicuna-7b-v1.3.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q2_K.bin",
    "parameterCount": "7B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "Vicuna",
    "description": "Original llama.cpp quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.",
    "filename": "vicuna-7b-v1.3.ggmlv3.q4_1.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q4_1.bin",
    "parameterCount": "7B",
    "quantization": "4-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "Vicuna",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "vicuna-7b-v1.3.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q8_0.bin",
    "parameterCount": "7B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "Vicuna",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "vicuna-33b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q2_K.bin",
    "parameterCount": "33B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "Vicuna",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "vicuna-33b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q8_0.bin",
    "parameterCount": "33B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "WizardLM Uncensored",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-7B-v1.0-Uncensored-ggml/resolve/main/wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
    "parameterCount": "7B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "WizardLM",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
    "parameterCount": "13B",
    "quantization": "4-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "WizardLM",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
    "parameterCount": "13B",
    "quantization": "8-bit",
    "labels": []
  },
  {
    "name": "WizardLM 30B",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "wizardlm-30b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q2_K.bin",
    "parameterCount": "30B",
    "quantization": "2-bit",
    "labels": [
      "k-quant"
    ]
  },
  {
    "name": "WizardLM 30B (8-bit)",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "wizardlm-30b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q8_0.bin",
    "parameterCount": "30B",
    "quantization": "8-bit",
    "labels": []
  }
]