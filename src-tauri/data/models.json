[
  {
    "name": "LLaMA 7B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-7b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q2_K.bin"
  },
  {
    "name": "LLaMA 7B (4-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-7b.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q4_K.bin"
  },
  {
    "name": "LLaMA 7B (8-bit)",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-7b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-7B-ggml/resolve/main/llama-7b.ggmlv3.q8_0.bin"
  },
  {
    "name": "LLaMA 13B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-13b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q2_K.bin"
  },
  {
    "name": "LLaMA 13B (4-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-13b.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q4_K_M.bin"
  },
  {
    "name": "LLaMA 13B (8-bit)",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-13b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/LLaMA-13B-ggml/resolve/main/llama-13b.ggmlv3.q8_0.bin"
  },
  {
    "name": "LLaMA 2 7B Chat (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "llama-2-7b-chat.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q2_K.bin"
  },
  {
    "name": "LLaMA 2 7B Chat (4-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "llama-2-7b-chat.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_M.bin"
  },
  {
    "name": "LLaMA 2 7B Chat (8-bit)",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "llama-2-7b-chat.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
  },
  {
    "name": "Vicuna 7B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "vicuna-7b-v1.3.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q2_K.bin"
  },
  {
    "name": "Vicuna 7B (8-bit)",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "vicuna-7b-v1.3.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-7B-v1.3-ggml/resolve/main/vicuna-7b-v1.3.ggmlv3.q8_0.bin"
  },
  {
    "name": "Vicuna 33B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "vicuna-33b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q2_K.bin"
  },
  {
    "name": "Vicuna 33B (8-bit)",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "vicuna-33b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/Vicuna-33B-v1.3-ggml/resolve/main/vicuna-33b.ggmlv3.q8_0.bin"
  },
  {
    "name": "WizardLM Uncensored 7B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-7B-v1.0-Uncensored-ggml/resolve/main/wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin"
  },
  {
    "name": "WizardLM 13B (4-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K",
    "filename": "wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin"
  },
  {
    "name": "WizardLM 13B (8-bit)",
    "description": "Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-13B-v1.1-ggml/resolve/main/wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
    "id": "wizardlm-13b-v1.1.ggmlv3.q8_0"
  },
  {
    "name": "WizardLM 30B (2-bit k-quant)",
    "description": "New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.",
    "filename": "wizardlm-30b.ggmlv3.q2_K.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q2_K.bin"
  },
  {
    "name": "WizardLM 30B (8-bit)",
    "description": "Original llama.cpp quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.",
    "filename": "wizardlm-30b.ggmlv3.q8_0.bin",
    "url": "https://huggingface.co/localmodels/WizardLM-30B-v1.0-ggml/resolve/main/wizardlm-30b.ggmlv3.q8_0.bin"
  }
]